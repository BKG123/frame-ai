#!/usr/bin/env python
"""
Run Frame AI evaluations and generate a report.

Usage:
    python -m evals.scripts.run_evals
    python -m evals.scripts.run_evals --verbose
"""

import sys
import argparse
from pathlib import Path
import subprocess
from datetime import datetime


def run_pytest(verbose: bool = False) -> tuple[int, str]:
    """
    Run pytest and capture results.

    Args:
        verbose: Whether to run in verbose mode

    Returns:
        Tuple of (return_code, output)
    """
    cmd = ["pytest", "evals/tests/", "-v"]

    if verbose:
        cmd.append("-vv")

    # Add JSON report
    cmd.extend(["--tb=short"])

    result = subprocess.run(cmd, capture_output=True, text=True)

    return result.returncode, result.stdout + result.stderr


def generate_report(output: str, return_code: int) -> str:
    """
    Generate a markdown report from pytest output.

    Args:
        output: pytest output text
        return_code: pytest return code

    Returns:
        Markdown formatted report
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Parse basic stats from output
    lines = output.split("\n")
    summary_line = ""
    for line in lines:
        if "passed" in line or "failed" in line:
            summary_line = line
            break

    report = f"""# Frame AI Evaluation Report

**Date:** {timestamp}
**Status:** {"✅ PASSED" if return_code == 0 else "❌ FAILED"}

## Summary

{summary_line}

## Test Categories

### 1. JSON Structure Tests
- Valid JSON format
- Required fields present
- Correct data types

### 2. Content Quality Tests
- Appropriate text lengths
- No placeholder text
- All sections meaningful

### 3. EXIF Handling Tests
- Mentions camera settings when available
- Graceful handling without EXIF

### 4. Completeness Tests
- All sections filled with unique content
- Meets minimum quality standards

## Detailed Output

```
{output}
```

---

*Generated by Frame AI Evaluation Suite*
"""

    return report


def save_report(report: str, results_dir: Path):
    """Save report to results directory."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = results_dir / f"eval_report_{timestamp}.md"

    with open(report_file, "w") as f:
        f.write(report)

    return report_file


def main():
    """Main entry point for evaluation runner."""
    parser = argparse.ArgumentParser(description="Run Frame AI evaluations")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    args = parser.parse_args()

    print("🧪 Running Frame AI Evaluations...")
    print("=" * 60)

    # Run tests
    return_code, output = run_pytest(verbose=args.verbose)

    # Generate report
    report = generate_report(output, return_code)

    # Save report
    results_dir = Path(__file__).parent / "results"
    results_dir.mkdir(exist_ok=True)
    report_file = save_report(report, results_dir)

    # Print summary
    print(output)
    print("\n" + "=" * 60)
    print(f"📊 Report saved to: {report_file}")

    if return_code == 0:
        print("✅ All evaluations passed!")
    else:
        print("❌ Some evaluations failed. Check the report for details.")

    return return_code


if __name__ == "__main__":
    sys.exit(main())
